{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba87e18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrigank/miniconda3/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3eff981",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2') # gpt2-medium, gpt2-large, gpt2-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26dbb45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight --> torch.Size([50257, 768])\n",
      "transformer.wpe.weight --> torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight --> torch.Size([768])\n",
      "transformer.h.0.ln_1.bias --> torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight --> torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias --> torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight --> torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.0.ln_2.weight --> torch.Size([768])\n",
      "transformer.h.0.ln_2.bias --> torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight --> torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias --> torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight --> torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.1.ln_1.weight --> torch.Size([768])\n",
      "transformer.h.1.ln_1.bias --> torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight --> torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias --> torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight --> torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.1.ln_2.weight --> torch.Size([768])\n",
      "transformer.h.1.ln_2.bias --> torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight --> torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias --> torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight --> torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.2.ln_1.weight --> torch.Size([768])\n",
      "transformer.h.2.ln_1.bias --> torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight --> torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias --> torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight --> torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.2.ln_2.weight --> torch.Size([768])\n",
      "transformer.h.2.ln_2.bias --> torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight --> torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias --> torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight --> torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.3.ln_1.weight --> torch.Size([768])\n",
      "transformer.h.3.ln_1.bias --> torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight --> torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias --> torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight --> torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.3.ln_2.weight --> torch.Size([768])\n",
      "transformer.h.3.ln_2.bias --> torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight --> torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias --> torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight --> torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.4.ln_1.weight --> torch.Size([768])\n",
      "transformer.h.4.ln_1.bias --> torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight --> torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias --> torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight --> torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.4.ln_2.weight --> torch.Size([768])\n",
      "transformer.h.4.ln_2.bias --> torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight --> torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias --> torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight --> torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.5.ln_1.weight --> torch.Size([768])\n",
      "transformer.h.5.ln_1.bias --> torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight --> torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias --> torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight --> torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.5.ln_2.weight --> torch.Size([768])\n",
      "transformer.h.5.ln_2.bias --> torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight --> torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias --> torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight --> torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.6.ln_1.weight --> torch.Size([768])\n",
      "transformer.h.6.ln_1.bias --> torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight --> torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias --> torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight --> torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.6.ln_2.weight --> torch.Size([768])\n",
      "transformer.h.6.ln_2.bias --> torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight --> torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias --> torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight --> torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.7.ln_1.weight --> torch.Size([768])\n",
      "transformer.h.7.ln_1.bias --> torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight --> torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias --> torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight --> torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.7.ln_2.weight --> torch.Size([768])\n",
      "transformer.h.7.ln_2.bias --> torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight --> torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias --> torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight --> torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.8.ln_1.weight --> torch.Size([768])\n",
      "transformer.h.8.ln_1.bias --> torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight --> torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias --> torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight --> torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.8.ln_2.weight --> torch.Size([768])\n",
      "transformer.h.8.ln_2.bias --> torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight --> torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias --> torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight --> torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.9.ln_1.weight --> torch.Size([768])\n",
      "transformer.h.9.ln_1.bias --> torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight --> torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias --> torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight --> torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.9.ln_2.weight --> torch.Size([768])\n",
      "transformer.h.9.ln_2.bias --> torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight --> torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias --> torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight --> torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.10.ln_1.weight --> torch.Size([768])\n",
      "transformer.h.10.ln_1.bias --> torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight --> torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias --> torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight --> torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.10.ln_2.weight --> torch.Size([768])\n",
      "transformer.h.10.ln_2.bias --> torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight --> torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias --> torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight --> torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.11.ln_1.weight --> torch.Size([768])\n",
      "transformer.h.11.ln_1.bias --> torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight --> torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias --> torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight --> torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias --> torch.Size([768])\n",
      "transformer.h.11.ln_2.weight --> torch.Size([768])\n",
      "transformer.h.11.ln_2.bias --> torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight --> torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias --> torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight --> torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias --> torch.Size([768])\n",
      "transformer.ln_f.weight --> torch.Size([768])\n",
      "transformer.ln_f.bias --> torch.Size([768])\n",
      "lm_head.weight --> torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "sd_hf = model.state_dict()\n",
    "for k, v in sd_hf.items():\n",
    "    print(f'{k} --> {v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5eb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "from transformers import GPT2Tokenizer\n",
    "# enc = tiktoken.get_encoding('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77989c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hi, I'm a language model,\"\n",
    "# tokens = enc.encode(prompt)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hi, I'm a language model,\"\n",
    "tok = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "input_ids = tok.encode(prompt, return_tensors='pt')\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd28c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(input_ids, max_new_tokens=30)\n",
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df8266",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.decode(output_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936fb455",
   "metadata": {},
   "source": [
    "## causal self attention elaborate affinity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a3e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a149b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "b,t,c = 2,3,4\n",
    "nh,hs = 2,2\n",
    "qkv = torch.randn((b,t,3*c))\n",
    "q,k,v = qkv.split(c, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "be67d07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(qkv[:,:,:c], q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ab7d0372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]), torch.Size([2, 3, 2, 2]), torch.Size([2, 2, 3, 2]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, q.view(b,t,nh,hs).shape, q.view(b,t,nh,hs).transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6d9a92fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "q.view(b,nh,t,hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cc03bf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = q.view(b,t,nh,hs).transpose(1,2)\n",
    "k = k.view(b,t,nh,hs).transpose(1,2)\n",
    "v = v.view(b,t,nh,hs).transpose(1,2)\n",
    "\n",
    "affinity_scores = (q @ k.transpose(-2,-1)) # (b,nh,t,t)\n",
    "affinity_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa372f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [1, 1, 0],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(t,t, dtype=torch.long))\n",
    "tril # (t,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6c29dca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000],\n",
       "          [0.9927, 0.0073, 0.0000],\n",
       "          [0.7182, 0.0590, 0.2228]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000],\n",
       "          [0.5729, 0.4271, 0.0000],\n",
       "          [0.3929, 0.3388, 0.2683]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000],\n",
       "          [0.9259, 0.0741, 0.0000],\n",
       "          [0.9809, 0.0021, 0.0169]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000],\n",
       "          [0.5949, 0.4051, 0.0000],\n",
       "          [0.1191, 0.4222, 0.4587]]]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affinity_scores = (q @ k.transpose(-2,-1)) # (b,nh,t,t)\n",
    "tril = torch.tril(torch.ones(t,t, dtype=torch.long))\n",
    "\n",
    "affinity_scores = affinity_scores.masked_fill(tril==0, float('-inf'))\n",
    "affinity_scores = torch.nn.functional.softmax(affinity_scores, dim=-1)\n",
    "affinity_scores # (b,nh,t,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d52df3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 2])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = affinity_scores @ v # (b,nh,t,t) @ (b,nh,t,hs) -> (b,nh,t,hs)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "251738b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1593,  0.3919],\n",
       "          [ 0.1635,  0.3851],\n",
       "          [-0.0017,  0.3043]],\n",
       "\n",
       "         [[-0.8028,  0.3271],\n",
       "          [-0.3842, -0.0507],\n",
       "          [-0.1121,  0.0312]]],\n",
       "\n",
       "\n",
       "        [[[-0.0119,  0.1332],\n",
       "          [ 0.0475,  0.1994],\n",
       "          [-0.0344,  0.1170]],\n",
       "\n",
       "         [[-1.2762,  1.7055],\n",
       "          [-1.0988,  1.5696],\n",
       "          [-0.3748,  0.2845]]]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0243d6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1593,  0.3919,  0.1635,  0.3851],\n",
       "         [-0.0017,  0.3043, -0.8028,  0.3271],\n",
       "         [-0.3842, -0.0507, -0.1121,  0.0312]],\n",
       "\n",
       "        [[-0.0119,  0.1332,  0.0475,  0.1994],\n",
       "         [-0.0344,  0.1170, -1.2762,  1.7055],\n",
       "         [-1.0988,  1.5696, -0.3748,  0.2845]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(b,t,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a9c12031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1593,  0.3919, -0.8028,  0.3271],\n",
       "         [ 0.1635,  0.3851, -0.3842, -0.0507],\n",
       "         [-0.0017,  0.3043, -0.1121,  0.0312]],\n",
       "\n",
       "        [[-0.0119,  0.1332, -1.2762,  1.7055],\n",
       "         [ 0.0475,  0.1994, -1.0988,  1.5696],\n",
       "         [-0.0344,  0.1170, -0.3748,  0.2845]]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.transpose(1,2).contiguous().view(b,t,c)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5036c6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1593,  0.3919,  0.1635,  0.3851],\n",
       "         [-0.0017,  0.3043, -0.8028,  0.3271],\n",
       "         [-0.3842, -0.0507, -0.1121,  0.0312]],\n",
       "\n",
       "        [[-0.0119,  0.1332,  0.0475,  0.1994],\n",
       "         [-0.0344,  0.1170, -1.2762,  1.7055],\n",
       "         [-1.0988,  1.5696, -0.3748,  0.2845]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac9791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
